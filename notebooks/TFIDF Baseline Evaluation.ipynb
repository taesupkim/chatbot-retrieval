{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "path = '/Users/kimts/Workspace/data/ubuntu/'\n",
    "train_df = pd.read_csv(os.path.join(path, \"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(path, \"test.csv\"))\n",
    "validation_df = pd.read_csv(os.path.join(path, \"valid.csv\"))\n",
    "y_test = np.zeros(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_recall(y, y_test, k=1):\n",
    "    num_examples = float(len(y))\n",
    "    num_correct = 0\n",
    "    for predictions, label in zip(y, y_test):\n",
    "        if label in predictions[:k]:\n",
    "            num_correct += 1\n",
    "    return num_correct/num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_random(context, utterances):\n",
    "    return np.random.choice(len(utterances), 10, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall @ (1, 10): 0.0991543\nRecall @ (2, 10): 0.199894\nRecall @ (5, 10): 0.501374\nRecall @ (10, 10): 1\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Random predictor\n",
    "y_random = [predict_random(test_df.Context[x], test_df.iloc[x,1:].values) for x in range(len(test_df))]\n",
    "for n in [1, 2, 5, 10]:\n",
    "    print(\"Recall @ ({}, 10): {:g}\".format(n, evaluate_recall(y_random, y_test, n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_path = '/Users/kimts/Workspace/code/chatbot-retrieval/data/glove.6B.100d.txt'\n",
    "with open(w2v_path, \"rb\") as lines:\n",
    "    w2v = {line.split()[0]: np.array(map(float, line.split()[1:]))\n",
    "           for line in lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class TFIDFPredictor:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "\n",
    "    def train(self, data):\n",
    "        self.vectorizer.fit(np.append(data.Context.values, data.Utterance.values))\n",
    "        \n",
    "    def predict(self, context, utterances):\n",
    "        # Convert context and utterances into tfidf vector\n",
    "        vector_context = self.vectorizer.transform([context])\n",
    "        vector_doc = self.vectorizer.transform(utterances)\n",
    "        # The dot product measures the similarity of the resulting vectors\n",
    "        result = np.dot(vector_doc, vector_context.T).todense()\n",
    "        result = np.asarray(result).flatten()\n",
    "        # Sort by top results and return the indices in descending order\n",
    "        return np.argsort(result, axis=0)[::-1]\n",
    "\n",
    "    \n",
    "class TFIDFGlovePredictor:\n",
    "    def __init__(self, word2vec):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.word2vec = word2vec\n",
    "\n",
    "    def train(self, data):\n",
    "        self.vectorizer.fit(np.append(data.Context.values, data.Utterance.values))\n",
    "        \n",
    "        max_idf = max(self.vectorizer.idf_)\n",
    "        \n",
    "        self.word2weight = defaultdict(lambda : max_idf, \n",
    "                                       [(w, self.vectorizer.idf_[i]) for w, i in self.vectorizer.vocabulary_.items()])\n",
    "\n",
    "    def predict(self, context, answer_list):\n",
    "        # tokenize\n",
    "        context_words = self.vectorizer.analyzer(context)\n",
    "        answer_words_list = [self.vectorizer.analyzer(answer) for answer in answer_list]\n",
    "\n",
    "        # weighted vector\n",
    "        context_vector = np.mean([self.word2vec[w]*self.word2weight[w]\n",
    "                                  for w in context_words if w in self.word2vec] or [np.zeros(100)], axis=0)        \n",
    "        \n",
    "        answer_vector_list = np.array([np.mean([self.word2vec[w]*self.word2weight[w]\n",
    "                                                for w in answer_words if w in self.word2vec] or [np.zeros(100)], axis=0)\n",
    "                                       for answer_words in answer_words_list])\n",
    "        result = cosine_similarity(context_vector.reshape((1, -1)), answer_vector_list).shape\n",
    "        result = np.asarray(result).flatten()\n",
    "        # Sort by top results and return the indices in descending order\n",
    "        return np.argsort(result, axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall @ (1, 10): 0.495032\nRecall @ (2, 10): 0.596882\nRecall @ (5, 10): 0.766121\nRecall @ (10, 10): 1\n"
     ]
    }
   ],
   "source": [
    "# Evaluate TFIDF predictor\n",
    "pred = TFIDFPredictor()\n",
    "pred.train(train_df)\n",
    "y = [pred.predict(test_df.Context[x], test_df.iloc[x,1:].values) for x in range(len(test_df))]\n",
    "for n in [1, 2, 5, 10]:\n",
    "    print(\"Recall @ ({}, 10): {:g}\".format(n, evaluate_recall(y, y_test, n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate TFIDF predictor\n",
    "w2v_pred = TFIDFGlovePredictor(w2v)\n",
    "w2v_pred.train(train_df)\n",
    "y = [w2v_pred.predict(test_df.Context[x], test_df.iloc[x, 1:].values) for x in range(len(test_df))]\n",
    "for n in [1, 2, 5, 10]:\n",
    "    print(\"Recall @ ({}, 10): {:g}\".format(n, evaluate_recall(y, y_test, n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 7 0 4 1 2 8 3 9 5]\n"
     ]
    }
   ],
   "source": [
    "print y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall @ (1, 10): 0.495032\nRecall @ (2, 10): 0.596882\nRecall @ (5, 10): 0.766121\nRecall @ (10, 10): 1\n"
     ]
    }
   ],
   "source": [
    "for n in [1, 2, 5, 10]:\n",
    "    print(\"Recall @ ({}, 10): {:g}\".format(n, evaluate_recall(y, y_test, n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}